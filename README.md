# ğŸ¥ LLMâ€‘Based Video Emotion Detection Web App

This project is a **Flaskâ€‘based web application** that analyzes a video to:

- ğŸ™ Transcribe spoken audio using **Whisper**
- ğŸ§  Generate a semantic description using a **Large Language Model (GPTâ€‘2)**
- ğŸ™‚ Detect the **dominant facial emotion** using **DeepFace**

The system combines **computer vision**, **speech processing**, and **LLMâ€‘based text generation** into a single endâ€‘toâ€‘end pipeline.

---

## ğŸš€ Features

- Upload and preview a video in the browser
- Speechâ€‘toâ€‘text transcription (Whisper)
- Emotion detection from video frames (DeepFace)
- Text generation using GPTâ€‘2 (LLM)
- Clean, professional UI built with HTML + CSS
- Emojiâ€‘enhanced emotion output for easy understanding

---

## ğŸ§  LLM Usage (Core Concept)

The **LLM component** of this project is isolated in `emotion_model.py`:

- **Whisper**: Converts speech in the video to text (foundation model)
- **GPTâ€‘2**: Generates a semantic description from the transcription
- **Tokenization & generation** are handled using HuggingFace Transformers

Flask (`app.py`) is used **only for orchestration and serving** the web interface.

---

## ğŸ—‚ Project Structure

```text
LLM-Video-Emotion-Detection/
â”‚
â”œâ”€â”€ app.py                  # Flask application (routes & orchestration)
â”œâ”€â”€ emotion_model.py        # ML + LLM pipeline (CORE FILE)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .gitignore              # Ignored files/folders
â”œâ”€â”€ README.md               # Project documentation
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html          # UI template
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ uploads/            # Temporary uploaded videos (ignored in git)
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository

```bash
git clone <your-repo-url>
cd LLM-Video-Emotion-Detection
```

### 2ï¸âƒ£ Create and activate virtual environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

> âš ï¸ Make sure you are using **Python 3.10 or 3.11**

---

## â–¶ï¸ Run the Application

```bash
python app.py
```

Then open your browser and go to:

```
http://127.0.0.1:5000
```

---

## â± Performance Notes

- Initial startup may take **30â€“90 seconds**
- This is expected because multiple **pretrained deep learning models** are loaded:
  - Whisper
  - GPTâ€‘2
  - DeepFace

- Once loaded, inference works normally

> This project is intended for **demonstration and academic purposes**.

---

## âš ï¸ Warnings

- TensorFlow and HuggingFace warnings during startup are **normal**
- Flask development server is used (not for production deployment)

---

## ğŸ§ª Dataset & Training

- No dataset is required at runtime
- The system uses **pretrained models only**
- Any experimental files (CSV / PKL) are excluded from the final project

---

## ğŸ›  Tech Stack

- **Backend**: Flask
- **LLM**: GPTâ€‘2 (HuggingFace Transformers)
- **Speech Model**: Whisper
- **Emotion Detection**: DeepFace (TensorFlow)
- **Frontend**: HTML, CSS, JavaScript

---

## ğŸ¯ Use Cases

- Human emotion analysis
- Multimodal AI demonstrations
- LLM + CV academic projects
- Interview / portfolio showcase

---

## ğŸ“Œ Disclaimer

This project is built for **learning and demonstration purposes** and is not optimized for production use.

---

## Deployment Note
Due to the memory requirements of Whisper, DeepFace, and LLM models, this project is intended to be run locally. Free-tier cloud platforms may run out of memory.


## ğŸ‘¤ Author

**Basith Rubani**

---

âœ¨ _A multimodal AI system combining vision, speech, and large language models._

