{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"CBFHSxl3Dxd8","executionInfo":{"status":"ok","timestamp":1746427711316,"user_tz":-330,"elapsed":522,"user":{"displayName":"Basith Rubani","userId":"17218659163026564369"}}},"outputs":[],"source":["#pip install ffmpeg-python"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rXOkpGRzAHQY","executionInfo":{"status":"ok","timestamp":1746427712006,"user_tz":-330,"elapsed":12,"user":{"displayName":"Basith Rubani","userId":"17218659163026564369"}}},"outputs":[],"source":["#!pip install git+https://github.com/openai/whisper.git  deepface ffmpeg-python"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My\\ Drive/LLM_MODEL_CLASSIFCATION (1)"],"metadata":{"id":"kewgF_AEy47i","colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"status":"error","timestamp":1746427833239,"user_tz":-330,"elapsed":121244,"user":{"displayName":"Basith Rubani","userId":"17218659163026564369"}},"outputId":"c9fc8005-fa00-4713-f2d9-16fedcb1d4a3"},"execution_count":3,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"mount failed","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c005a1406d6d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/My\\\\ Drive/LLM_MODEL_CLASSIFCATION (1)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ia7quGb869s0","executionInfo":{"status":"aborted","timestamp":1746427833240,"user_tz":-330,"elapsed":11,"user":{"displayName":"Basith Rubani","userId":"17218659163026564369"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import cv2\n","from deepface import DeepFace\n","import whisper\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","import ffmpeg\n","\n","# Initialize GPT-2 model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Step 1: Extract audio from the video\n","def extract_audio_from_video(video_path, audio_path=\"temp_audio.wav\"):\n","    ffmpeg.input(video_path).output(audio_path).run(overwrite_output=True)\n","\n","# Step 2: Transcribe audio to text using Whisper\n","def transcribe_audio(audio_path):\n","    whisper_model = whisper.load_model(\"base\")\n","    result = whisper_model.transcribe(audio_path)\n","    return result['text']\n","\n","def get_highest_non_neutral_emotion(emotion_dict):\n","    # Remove 'neutral' from the emotion dictionary\n","    if 'neutral' in emotion_dict:\n","        del emotion_dict['neutral']\n","    # Find the emotion with the highest probability\n","    dominant_emotion = max(emotion_dict, key=emotion_dict.get)\n","    return dominant_emotion, emotion_dict[dominant_emotion]\n","\n","def analyze_video_frames(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    emotions_across_frames = []\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Analyze frame with DeepFace\n","        result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n","\n","        # Get highest non-neutral emotion\n","        dominant_emotion, emotion_prob = get_highest_non_neutral_emotion(result[0]['emotion'])\n","        emotions_across_frames.append((dominant_emotion, emotion_prob))\n","\n","    cap.release()\n","\n","    # Find the dominant emotion with the highest probability across all frames\n","    if emotions_across_frames:\n","        overall_dominant_emotion, highest_prob = max(emotions_across_frames, key=lambda x: x[1])\n","        return overall_dominant_emotion, highest_prob\n","    else:\n","        return None, None\n","\n","# Step 4: Generate a textual description using GPT-2\n","def generate_description(text):\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model.generate(inputs, max_length=150, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n","    description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return description\n","\n","def process_video(video_path):\n","    # Define paths\n","    audio_path = \"temp_audio.wav\"\n","\n","    # Extract audio from the video\n","    extract_audio_from_video(video_path, audio_path)\n","\n","    # Transcribe the audio to text\n","    transcription = transcribe_audio(audio_path)\n","\n","    # Analyze the video for the most prominent emotion\n","    dominant_emotion, highest_prob = analyze_video_frames(video_path)\n","\n","    # Generate a textual description using GPT-2\n","    description = generate_description(transcription)\n","\n","    # Incorporate emotion into the enhanced description\n","    enhanced_description = f\"{description} The person seems to be displaying '{dominant_emotion}' emotion.\"\n","\n","    # Clean up\n","    os.remove(audio_path)\n","\n","    return transcription, enhanced_description\n","\n","def process_emotion_dataset(root_folder):\n","    # Create an empty list to store the results\n","    results = []\n","\n","    # Iterate through the emotion folders\n","    for label in os.listdir(root_folder):\n","        folder_path = os.path.join(root_folder, label)\n","\n","\n","        if os.path.isdir(folder_path):\n","            # Iterate through each video file in the emotion folder\n","            for video_file in os.listdir(folder_path):\n","                video_path = os.path.join(folder_path, video_file)\n","\n","\n","                if os.path.isfile(video_path):\n","                    # Process the video\n","                    transcription, enhanced_description = process_video(video_path)\n","                    label=video_file[0:2]\n","                    if(label==\"an\"):\n","                        label=\"angry\"\n","                    elif(label==\"di\"):\n","                        label=\"disguise\"\n","                    elif(label==\"fe\"):\n","                        label=\"fear\"\n","                    elif(label==\"su\"):\n","                        label=\"surprise\"\n","                    elif(label==\"sa\"):\n","                        label=\"sad\"\n","                    elif(label==\"ha\"):\n","                        label=\"happy\"\n","\n","\n","\n","                    # Append the results to the list\n","                    results.append({\n","                        \"label\": label,\n","                        \"textual_description\": transcription,\n","                        \"enhanced_textual_description\": enhanced_description\n","                    })\n","                    print(video_path)\n","\n","    # Convert the list to a pandas DataFrame\n","    df = pd.DataFrame(results)\n","\n","    # Save the DataFrame to a CSV file (optional)\n","    df.to_csv(\"emotion_dataset_results.csv\", index=False)\n","\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bck6CJ2QywqA","executionInfo":{"status":"aborted","timestamp":1746427833241,"user_tz":-330,"elapsed":11,"user":{"displayName":"Basith Rubani","userId":"17218659163026564369"}}},"outputs":[],"source":["# Example usage\n","root_folder = \"Dataset\"  #\n","df = process_emotion_dataset(root_folder)\n","\n","print(df)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}